# UR5e-Gesture-control
Controlling of UR5e Robot with Gesture control using Mediapipe

â€œWave It, Move It!â€ âœ‹ğŸ¤–

Youtube Link: https://youtu.be/HPGEiMZfP5M


Another Weekend Wrap: I built a foundational-level gestureâ€‘controlled UR5e demo using MediaPipe and Python, laying the groundwork for more advanced AI and machineâ€‘vision integrations.



ğŸ“¹ Captured realâ€‘time hand landmarks with MediaPipeâ€™s highâ€‘fidelity handâ€‘tracking pipeline.

ğŸ–¥ï¸ Processed landmarks in Python to interpret five discrete commands (Home, Up, Down, Left, and Right).

ğŸ”Œ Streamed URScript commands over a TCP socket to the UR5e, enabling intuitive teleoperation.

ğŸ¯ Ensured safe motion with speedâ€‘limiting.



Key technical highlights:

MediaPipe Hands for robust, lowâ€‘latency landmark detection.

URScript via socket: speedl() for smooth Cartesian movements.

Modular architecture for easy scaling.



While this demo remains fun and foundational, itâ€™s a crucial stepping stone toward industrial use cases:

Select tools or parts on an assembly line with a simple finger point

Trigger safety stops instantly with a clear â€œstopâ€ gesture

AIâ€‘driven gesture recognition using convolutional neural networks.

Reinforcement learning to teach the robot adaptive grasping and motion strategies.

ROSâ€‘based frameworks for multiâ€‘modal control (vision + gesture + voice).



With further work on integration with PLCs or ROS, gesture control could become a useful layer of humanâ€‘robot collaboration in smart manufacturing.



Please excuse any stutter. This code is running on a standard laptop and hasnâ€™t been performance-tuned.
